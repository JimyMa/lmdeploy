import os
import torch
import torch.nn.functional as F
from typing import Optional, Dict
import torch.distributed as dist
from torch.multiprocessing import spawn

# -------------------------- å…¨å±€é…ç½® --------------------------
NUM_HEADS     = 32
HEAD_DIM      = 576
KV_NUM_HEADS  = 8
KV_DIM        = HEAD_DIM
MAX_SEQ_LEN   = 1024

def empty_tensor(shape: tuple, device: torch.device) -> torch.Tensor:
    return torch.empty(shape, dtype=torch.float32, device=device)

def pad_to_max_len(tensor: torch.Tensor, max_len: int, dim: int = 0) -> torch.Tensor:
    pad_len = max_len - tensor.shape[dim]
    if pad_len <= 0:
        return tensor
    pad_shape = list(tensor.shape)
    pad_shape[dim] = pad_len
    pad = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)
    return torch.cat([tensor, pad], dim=dim)

def get_fixed_qkv(batch_size: int, device: torch.device) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """ä»…ç”ŸæˆQï¼›K/Vè¿”å›ç©ºå¼ é‡ä¿æŒæ¥å£å…¼å®¹"""
    q = torch.zeros(batch_size, NUM_HEADS, HEAD_DIM, device=device)
    for b in range(batch_size):
        for h in range(NUM_HEADS):
            for d in range(HEAD_DIM):
                q[b, h, d] = b*100 + h*10 + d
    k = torch.empty(batch_size, MAX_SEQ_LEN, KV_NUM_HEADS, KV_DIM, device=device)
    v = torch.empty(batch_size, MAX_SEQ_LEN, KV_NUM_HEADS, KV_DIM, device=device)
    return q, k, v

def dummy_merge(results_list: list[torch.Tensor]) -> torch.Tensor:
    """å‚è€ƒç¤ºä¾‹ä»£ç çš„åˆå¹¶é€»è¾‘ï¼šå¯¹æ¯ä¸ªSPè¯·æ±‚çš„æ‰€æœ‰Rankç»“æœå–å¹³å‡"""
    if not results_list:
        raise ValueError("åˆå¹¶ç»“æœåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    return torch.stack(results_list).mean(dim=0)

# -------------------------- forwardï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰ --------------------------
def forward(
    hidden_states: torch.Tensor,
    sp_groups_info: Optional[Dict] = None,
    sp_comm_groups: Optional[Dict] = None,
) -> tuple[torch.Tensor, Dict]:
    rank   = dist.get_rank()
    device = hidden_states.device
    meta   = {}

    print(f"\n========== [RANK {rank}] START ==========",flush=True)

    # 1) è·å– Q
    query_states, _, _ = get_fixed_qkv(hidden_states.shape[0], device)
    print(f"[RANK {rank}] query_states  -> {query_states.shape}",flush=True)

    # 2) æ‹†åˆ†æœ¬åœ° / SP è¯·æ±‚
    local_batches, sp_batches = [], []
    for b in range(query_states.shape[0]):
        info = sp_groups_info.get(b, {'enabled': False})
        if not info['enabled']:
            local_batches.append(b)
        elif rank in info['group']:
            sp_batches.append((b, info))
    print(f"[RANK {rank}] local_batches = {local_batches}",flush=True)
    print(f"[RANK {rank}] sp_batches    = {[tpl[0] for tpl in sp_batches]}",flush=True)

    # 3) å»ºç«‹é€šä¿¡ç»„
    sp_groups = {}
    for b, info in sp_batches:
        key = tuple(sorted(info['group']))
        if key not in sp_groups:
            sp_groups[key] = []
        sp_groups[key].append((b, info))
    assert len(sp_groups) <= 1
    print(f"[RANK {rank}] sp_groups keys = {list(sp_groups.keys())}",flush=True)

    # -------------------------- å…³é”®ä¿®æ”¹1ï¼šè·å–æ‰€æœ‰å·²å­˜åœ¨çš„SPé€šä¿¡ç»„Key --------------------------
    # å³ä½¿æ— SPè¯·æ±‚ï¼Œä¹Ÿéœ€å¤„ç†sp_comm_groupsä¸­æ‰€æœ‰SPç»„ï¼ˆé¿å…é€šä¿¡é—æ¼ï¼‰
    all_sp_group_keys = list(sp_comm_groups.keys()) if sp_comm_groups else []
    # è¡¥å……å½“å‰Rankè‡ªèº«çš„SPç»„Keyï¼ˆé˜²æ­¢sp_comm_groupsæœªé¢„åˆ›å»ºï¼‰
    for key in sp_groups.keys():
        if key not in all_sp_group_keys:
            all_sp_group_keys.append(key)
    print(f"[RANK {rank}] éœ€å¤„ç†çš„æ‰€æœ‰SPç»„ = {all_sp_group_keys}",flush=True)

    # 4) All-Gather Qï¼šéå†æ‰€æœ‰SPç»„ï¼ˆæ— è®ºæ˜¯å¦æœ‰æœ¬åœ°SPè¯·æ±‚ï¼‰
    all_sp_q, sp_batch_indices = [], {}
    for key in all_sp_group_keys:
        # è·å–/åˆ›å»ºé€šä¿¡ç»„
        comm = sp_comm_groups.get(key)
        if comm is None:
            comm = dist.new_group(list(key))
            sp_comm_groups[key] = comm
        group_ranks = list(key)

        # -------------------------- å…³é”®ä¿®æ”¹2ï¼šè¡¥å…¨æœ¬åœ°SPæ•°æ®ï¼ˆæ— è¯·æ±‚åˆ™ä¸ºç©ºï¼‰ --------------------------
        reqs = sp_groups.get(key, [])  # æ— è¯·æ±‚åˆ™ä¸ºç©ºåˆ—è¡¨
        local_q_list = [query_states[b] for b, _ in reqs] if reqs else []
        local_q = torch.stack(local_q_list) if local_q_list else \
                  torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
        local_cnt = len(local_q_list)  # 0æˆ–æ­£æ•´æ•°
        print(f"[RANK {rank}] SPç»„ {key} - local_q shape: {local_q.shape}, æœ¬åœ°è¯·æ±‚æ•°: {local_cnt}",flush=True)

        # åŒæ­¥ç»„å†…æ‰€æœ‰Rankçš„è¯·æ±‚æ•°ï¼ˆæ‰€æœ‰Rankå¿…é¡»å‚ä¸ï¼Œå³ä½¿local_cnt=0ï¼‰
        local_cnt_tensor = torch.tensor([local_cnt], dtype=torch.long, device=device)
        cnt_list_tensor = [torch.empty_like(local_cnt_tensor) for _ in group_ranks]
        dist.all_gather(cnt_list_tensor, local_cnt_tensor, group=comm)
        cnt_list = [t.item() for t in cnt_list_tensor]
        max_cnt = max(cnt_list) if cnt_list else 0
        print(f"[RANK {rank}] SPç»„ {key} - ç»„å†…å„Rankè¯·æ±‚æ•°: {cnt_list}, æœ€å¤§é•¿åº¦: {max_cnt}",flush=True)

        # å¡«å……ç©ºå¼ é‡ï¼ˆç¡®ä¿æ‰€æœ‰Rankå‘é€æ•°æ®é•¿åº¦ä¸€è‡´ï¼‰
        padded_q = pad_to_max_len(local_q, max_cnt, dim=0)
        # All-Gatherï¼šæ‰€æœ‰Rankå¿…é¡»æ‰§è¡Œï¼ˆç©ºæ•°æ®ä¹Ÿéœ€å‘é€ï¼‰
        gathered = torch.empty(len(group_ranks), max_cnt, NUM_HEADS, HEAD_DIM,
                               dtype=padded_q.dtype, device=device)
        dist.all_gather_into_tensor(gathered.view(-1), padded_q.contiguous(), group=comm)
        print(f"[RANK {rank}] SPç»„ {key} - gathered_q shape: {gathered.shape}",flush=True)

        # æ‰å¹³åŒ–SPè¯·æ±‚ï¼ˆè¿‡æ»¤ç©ºæ•°æ®ï¼‰
        flat = []
        for i, c in enumerate(cnt_list):
            if c > 0:
                flat.append(gathered[i, :c])
        flat = torch.cat(flat) if flat else torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
        all_sp_q.append(flat)
        sp_batch_indices[key] = [b for b, _ in reqs] if reqs else []

        # -------------------------- å…³é”®ä¿®æ”¹3ï¼šè¡¥å…¨SPç»„å…ƒä¿¡æ¯ï¼ˆæ— è¯·æ±‚ä¹Ÿéœ€è®°å½•ï¼‰ --------------------------
        master_rank = reqs[0][1]['master_rank'] if reqs else group_ranks[0]
        meta[key] = {
            'group_ranks': group_ranks,
            'master_rank': master_rank,
            'local_batch_count': local_cnt,  # å½“å‰Rankçš„SPè¯·æ±‚æ•°ï¼ˆ0æˆ–æ­£æ•´æ•°ï¼‰
            'all_batch_counts': cnt_list,    # ç»„å†…æ‰€æœ‰Rankçš„è¯·æ±‚æ•°
            'is_master': (rank == master_rank)
        }

    # 5) åˆå¹¶æ‰€æœ‰ Q
    local_q = query_states[local_batches] if local_batches else \
              torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
    sp_q    = torch.cat(all_sp_q, dim=0) if all_sp_q else \
              torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
    all_q   = torch.cat([local_q, sp_q], dim=0)
    print(f"[RANK {rank}] local_q={local_q.shape}, sp_q={sp_q.shape}, all_q={all_q.shape}",flush=True)

    # 6) æ¨¡æ‹Ÿæ³¨æ„åŠ›è®¡ç®—ï¼ˆå ä½ï¼‰
    attn_output = torch.randn(all_q.shape[0], NUM_HEADS, KV_DIM, device=device)
    print(f"[RANK {rank}] attn_output shape: {attn_output.shape}",flush=True)

    # 7) æ‹†åˆ†ç»“æœ
    local_cnt   = local_q.shape[0]
    local_res   = attn_output[:local_cnt]
    sp_res      = attn_output[local_cnt:]
    print(f"[RANK {rank}] local_res={local_res.shape}, sp_res={sp_res.shape}",flush=True)

    # 8) All2All æ‹†åˆ†SPç»“æœï¼šéå†æ‰€æœ‰SPç»„ï¼ˆæ— è®ºæ˜¯å¦æœ‰æœ¬åœ°SPè¯·æ±‚ï¼‰
    final_sp_parts = []
    sp_ptr = 0
    for key in all_sp_group_keys:
        comm = sp_comm_groups[key]
        m = meta[key]
        group_ranks = m['group_ranks']
        all_cnts = m['all_batch_counts']
        my_sp_cnt = m['local_batch_count']
        rank_idx = group_ranks.index(rank)
        total_sp = sum(all_cnts)

        # -------------------------- å…³é”®ä¿®æ”¹4ï¼šè¡¥å…¨SPç»“æœåˆ‡ç‰‡ï¼ˆæ— è¯·æ±‚åˆ™ä¸ºç©ºï¼‰ --------------------------
        slice_sp = sp_res[sp_ptr:sp_ptr + total_sp] if total_sp > 0 else \
                   torch.empty(0, NUM_HEADS, KV_DIM, device=device)
        sp_ptr += total_sp
        print(f"[RANK {rank}] SPç»„ {key} - slice_sp shape: {slice_sp.shape}",flush=True)

        # æ„é€ å‘é€/æ¥æ”¶é•¿åº¦ï¼ˆæ— è¯·æ±‚æ—¶recv_cnts=0ï¼‰
        send_cnts = all_cnts
        recv_cnts = [my_sp_cnt for _ in group_ranks]
        print(f"[RANK {rank}] SPç»„ {key} - send_cnts={send_cnts}, recv_cnts={recv_cnts}",flush=True)

        # æ„é€ å‘é€åˆ—è¡¨ï¼ˆç©ºæ•°æ®ä¹Ÿéœ€æ„é€ ç©ºå¼ é‡ï¼‰
        send_list, pos = [], 0
        for c in send_cnts:
            end = pos + c
            send_tensor = slice_sp[pos:end].contiguous() if (c > 0 and slice_sp.numel() > 0) else \
                          torch.empty(0, NUM_HEADS, KV_DIM, device=device)
            send_list.append(send_tensor)
            pos = end

        # æ„é€ æ¥æ”¶åˆ—è¡¨ï¼ˆç©ºæ•°æ®ä¹Ÿéœ€æ„é€ ç©ºå¼ é‡ï¼‰
        recv_list = []
        for c in recv_cnts:
            recv_tensor = torch.empty(c, NUM_HEADS, KV_DIM, device=device) if c > 0 else \
                          torch.empty(0, NUM_HEADS, KV_DIM, device=device)
            recv_list.append(recv_tensor)

        # -------------------------- å…³é”®ä¿®æ”¹5ï¼šæ‰€æœ‰Rankå¿…é¡»æ‰§è¡ŒAll-to-All --------------------------
        dist.all_to_all(recv_list, send_list, group=comm)

        # åˆå¹¶SPç»“æœï¼ˆæ— è¯·æ±‚åˆ™æ·»åŠ ç©ºå¼ é‡ï¼‰
        merged_sp_tensor = torch.empty(0, NUM_HEADS, KV_DIM, device=device)
        if my_sp_cnt > 0:
            # åˆå¹¶æ¥æ”¶çš„éç©ºå¼ é‡
            sp_results_received = [t for t in recv_list if t.numel() > 0]
            sp_results_received = torch.cat(sp_results_received, dim=0) if sp_results_received else \
                                  torch.empty(0, NUM_HEADS, KV_DIM, device=device)
            print(f"[RANK {rank}] SPç»„ {key} - æ¥æ”¶ç»“æœ shape: {sp_results_received.shape}",flush=True)

            # åˆ†é…å¹¶åˆå¹¶æ¯ä¸ªSPè¯·æ±‚çš„ç»“æœ
            sp_request_results = [[] for _ in range(my_sp_cnt)]
            current_pos = 0
            for i in range(len(group_ranks)):
                recv_c = recv_cnts[i]
                if recv_c > 0:
                    end_pos = current_pos + recv_c
                    for req_idx in range(recv_c):
                        sp_request_results[req_idx].append(sp_results_received[current_pos + req_idx])
                    current_pos = end_pos

            merged_sp_results = [dummy_merge(req_res) for req_res in sp_request_results]
            merged_sp_tensor = torch.stack(merged_sp_results, dim=0)
            print(f"[RANK {rank}] SPç»„ {key} - åˆå¹¶ç»“æœ shape: {merged_sp_tensor.shape}",flush=True)
        else:
            print(f"[RANK {rank}] SPç»„ {key} - æ— SPè¯·æ±‚ï¼Œè·³è¿‡åˆå¹¶",flush=True)

        final_sp_parts.append(merged_sp_tensor)

    # 9) æœ€ç»ˆè¾“å‡º
    final_sp = torch.cat(final_sp_parts) if final_sp_parts else \
               torch.empty(0, NUM_HEADS, KV_DIM, device=device)
    final    = torch.cat([local_res, final_sp], dim=0)
    print(f"[RANK {rank}] final_sp={final_sp.shape}, final={final.shape}",flush=True)
    print(f"========== [RANK {rank}] DONE ==========\n",flush=True)
    return final, meta

# -------------------------- åˆ†å¸ƒå¼æµ‹è¯• --------------------------
def init_distributed(rank: int, world_size: int):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12362'
    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def verify_results(rank: int, final_output: torch.Tensor, sp_groups_info: Dict) -> bool:
    # è®¡ç®—é¢„æœŸç»“æœæ•°é‡ï¼šæœ¬åœ°è¯·æ±‚æ•° + å½“å‰Rankçš„SPè¯·æ±‚æ•°
    expected_cnt = 0
    for b, info in sp_groups_info.items():
        if not info['enabled']:
            expected_cnt += 1
        elif rank in info['group']:
            expected_cnt += 1
    expected_shape = (expected_cnt, NUM_HEADS, KV_DIM)

    # æ ¡éªŒå½¢çŠ¶å’Œæœ‰æ•ˆæ€§
    if final_output.shape != expected_shape:
        print(f"âŒ Rank {rank} å½¢çŠ¶ä¸åŒ¹é…ï¼šæœŸæœ› {expected_shape}, å®é™… {final_output.shape}",flush=True)
        return False
    if final_output.numel() > 0 and torch.all(final_output == 0):
        print(f"âŒ Rank {rank} è¾“å‡ºå…¨é›¶ï¼ˆæ— æ•ˆï¼‰",flush=True)
        return False
    print(f"âœ… Rank {rank} æ ¡éªŒé€šè¿‡",flush=True)
    return True

def test_forward(rank: int, world_size: int):
    init_distributed(rank, world_size)
    device = torch.device(f'cuda:{rank}')

    # æµ‹è¯•ç”¨ä¾‹ï¼šRank 3æ— SPè¯·æ±‚ï¼ˆä»…2ä¸ªæœ¬åœ°è¯·æ±‚ï¼‰
    sp_groups_info_list = [
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 0},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 0},
         2: {'enabled': False}, 3: {'enabled': False}},  # Rank 0ï¼š2SP+2Local
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 1},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 1},
         2: {'enabled': False}, 3: {'enabled': False}},  # Rank 1ï¼š2SP+2Local
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 2},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 2},
         2: {'enabled': False}, 3: {'enabled': False}},  # Rank 2ï¼š2SP+2Local
        {0: {'enabled': False}, 1: {'enabled': False}}   # Rank 3ï¼š0SP+2Localï¼ˆå…³é”®æµ‹è¯•ï¼‰
    ]

    # ç”Ÿæˆè¾“å…¥æ•°æ®
    bs_current_rank = len(sp_groups_info_list[rank])
    hidden_states = torch.randn(bs_current_rank, MAX_SEQ_LEN, HEAD_DIM, device=device)

    # é¢„åˆ›å»ºSPé€šä¿¡ç»„ï¼ˆæ‰€æœ‰Rankéƒ½éœ€çŸ¥é“SPç»„[0,1,2,3]ï¼‰
    sp_comm_groups = {}
    sp_group_key = tuple(sorted([0,1,2,3]))
    if sp_group_key not in sp_comm_groups:
        comm = dist.new_group(ranks=[0,1,2,3])
        sp_comm_groups[sp_group_key] = comm

    # æ‰§è¡Œforward
    final_output, _ = forward(
        hidden_states=hidden_states,
        sp_groups_info=sp_groups_info_list[rank],
        sp_comm_groups=sp_comm_groups
    )

    # æ ¡éªŒç»“æœ
    ok = verify_results(rank, final_output, sp_groups_info_list[rank])
    dist.barrier()

    # æ±‡æ€»æ ¡éªŒç»“æœï¼ˆRank 0æ”¶é›†æ‰€æœ‰ç»“æœï¼‰
    if rank == 0:
        all_ok = torch.tensor(1, device='cuda:0')
        for r in range(1, world_size):
            buf = torch.tensor(0, device='cuda:0')
            dist.recv(buf, src=r)
            all_ok &= buf
        print("\nğŸ‰ æ‰€æœ‰Rankæ ¡éªŒé€šè¿‡ï¼" if all_ok else "\nâŒ éƒ¨åˆ†Rankæ ¡éªŒå¤±è´¥ï¼",flush=True)
    else:
        dist.send(torch.tensor(1 if ok else 0, device='cuda'), dst=0)

    dist.barrier()
    dist.destroy_process_group()

def main():
    WORLD_SIZE = 4  # 4å¡åˆ†å¸ƒå¼æµ‹è¯•
    spawn(fn=test_forward, args=(WORLD_SIZE,), nprocs=WORLD_SIZE, join=True)

if __name__ == '__main__':
    main()