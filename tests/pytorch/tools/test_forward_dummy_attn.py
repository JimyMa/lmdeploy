import os
import torch
import torch.nn.functional as F
from typing import Optional, Dict
import torch.distributed as dist
from torch.multiprocessing import spawn

# -------------------------- å…¨å±€é…ç½® --------------------------
NUM_HEADS     = 32
HEAD_DIM      = 576
KV_NUM_HEADS  = 8
KV_DIM        = HEAD_DIM
MAX_SEQ_LEN   = 1024
BATCH_SIZE    = 4

def empty_tensor(shape: tuple, device: torch.device) -> torch.Tensor:
    return torch.empty(shape, dtype=torch.float32, device=device)

def pad_to_max_len(tensor: torch.Tensor, max_len: int, dim: int = 0) -> torch.Tensor:
    pad_len = max_len - tensor.shape[dim]
    if pad_len <= 0:
        return tensor
    pad_shape = list(tensor.shape)
    pad_shape[dim] = pad_len
    pad = torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)
    return torch.cat([tensor, pad], dim=dim)

def get_fixed_qkv(device: torch.device) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """ä»…ç”ŸæˆQï¼›K/Vè¿”å›ç©ºå¼ é‡ä¿æŒæ¥å£å…¼å®¹"""
    q = torch.zeros(BATCH_SIZE, NUM_HEADS, HEAD_DIM, device=device)
    for b in range(BATCH_SIZE):
        for h in range(NUM_HEADS):
            for d in range(HEAD_DIM):
                q[b, h, d] = b*100 + h*10 + d
    k = torch.empty(BATCH_SIZE, MAX_SEQ_LEN, KV_NUM_HEADS, KV_DIM, device=device)
    v = torch.empty(BATCH_SIZE, MAX_SEQ_LEN, KV_NUM_HEADS, KV_DIM, device=device)
    return q, k, v

def dummy_merge(results_list: list[torch.Tensor]) -> torch.Tensor:
    """å‚è€ƒç¤ºä¾‹ä»£ç çš„åˆå¹¶é€»è¾‘ï¼šå¯¹æ¯ä¸ªSPè¯·æ±‚çš„æ‰€æœ‰Rankç»“æœå–å¹³å‡"""
    if not results_list:
        raise ValueError("åˆå¹¶ç»“æœåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    # å †å æ‰€æœ‰Rankçš„ç»“æœå¹¶æŒ‰Rankç»´åº¦å–å¹³å‡ï¼ˆdim=0ä¸ºRankç»´åº¦ï¼‰
    return torch.stack(results_list).mean(dim=0)

# -------------------------- forward --------------------------
def forward(
    hidden_states: torch.Tensor,
    sp_groups_info: Optional[Dict] = None,
    sp_comm_groups: Optional[Dict] = None,
) -> tuple[torch.Tensor, Dict]:
    rank   = dist.get_rank()
    device = hidden_states.device
    meta   = {}

    print(f"\n========== [RANK {rank}] START ==========",flush=True)

    # 1) è·å– Q
    query_states, _, _ = get_fixed_qkv(device)
    print(f"[RANK {rank}] query_states  -> {query_states.shape}",flush=True)

    # 2) æ‹†åˆ†æœ¬åœ° / SP è¯·æ±‚
    local_batches, sp_batches = [], []
    for b in range(BATCH_SIZE):
        info = sp_groups_info.get(b, {'enabled': False})
        if not info['enabled']:
            local_batches.append(b)
        elif rank in info['group']:
            sp_batches.append((b, info))
    print(f"[RANK {rank}] local_batches = {local_batches}",flush=True)
    print(f"[RANK {rank}] sp_batches    = {[tpl[0] for tpl in sp_batches]}",flush=True)

    # 3) å»ºç«‹é€šä¿¡ç»„
    sp_groups = {}
    for b, info in sp_batches:
        key = tuple(sorted(info['group']))
        if key not in sp_groups:
            sp_groups[key] = []
        sp_groups[key].append((b, info))
    assert len(sp_groups) <= 1
    print(f"[RANK {rank}] sp_groups keys = {list(sp_groups.keys())}",flush=True)

    # 4) All-Gather Q
    all_sp_q, sp_batch_indices = [], {}
    for key, reqs in sp_groups.items():
        comm = sp_comm_groups.get(key)
        if comm is None:
            comm = dist.new_group(list(key))
            sp_comm_groups[key] = comm

        local_q_list = [query_states[b] for b, _ in reqs]
        local_q      = torch.stack(local_q_list) if local_q_list else \
                       torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
        print(f"[RANK {rank}] local_q for group {key} -> {local_q.shape}",flush=True)

        # åŒæ­¥ batch æ•°
        local_cnt = torch.tensor([len(local_q_list)], dtype=torch.long, device=device)
        cnt_list  = [torch.empty_like(local_cnt) for _ in key]
        dist.all_gather(cnt_list, local_cnt, group=comm)
        cnt_list  = [c.item() for c in cnt_list]
        max_cnt   = max(cnt_list)
        print(f"[RANK {rank}] group {key} cnt_list={cnt_list}, max_cnt={max_cnt}",flush=True)

        # å¡«å……+All-Gather
        padded_q = pad_to_max_len(local_q, max_cnt, dim=0)
        gathered = torch.empty(len(key), max_cnt, NUM_HEADS, HEAD_DIM,
                               dtype=padded_q.dtype, device=device)
        dist.all_gather_into_tensor(gathered.view(-1), padded_q.contiguous(), group=comm)
        print(f"[RANK {rank}] gathered_q shape -> {gathered.shape}",flush=True)

        # æ‰å¹³åŒ–
        flat = []
        for i, c in enumerate(cnt_list):
            if c > 0:
                flat.append(gathered[i, :c])
        flat = torch.cat(flat) if flat else torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
        print(f"[RANK {rank}] flat_sp_q after cat -> {flat.shape}",flush=True)

        all_sp_q.append(flat)
        # è®°å½•å½“å‰Rankåœ¨è¯¥SPç»„ä¸­çš„æ‰¹æ¬¡ç´¢å¼•ï¼ˆåç»­ç”¨äºåŒ¹é…SPè¯·æ±‚ï¼‰
        sp_batch_indices[key] = [b for b, _ in reqs]

        meta[key] = {
            'group_ranks': list(key),
            'master_rank': reqs[0][1]['master_rank'],
            'local_batch_count': len(local_q_list),  # å½“å‰Rankçš„SPè¯·æ±‚æ•°ï¼ˆmy_sp_cntï¼‰
            'all_batch_counts': cnt_list,
            'is_master': (rank == reqs[0][1]['master_rank'])
        }

    # 5) åˆå¹¶æ‰€æœ‰ Q
    local_q = query_states[local_batches] if local_batches else \
              torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
    sp_q    = torch.cat(all_sp_q, dim=0) if all_sp_q else \
              torch.empty(0, NUM_HEADS, HEAD_DIM, device=device)
    all_q   = torch.cat([local_q, sp_q], dim=0)
    print(f"[RANK {rank}] local_q={local_q.shape}, sp_q={sp_q.shape}, all_q={all_q.shape}",flush=True)

    # 6) åŸ K/V é€»è¾‘å·²åˆ é™¤
    # 7) ç”¨ä¸€æ¬¡çŸ©é˜µä¹˜å ä½ï¼ˆæ¨¡æ‹Ÿæ³¨æ„åŠ›è®¡ç®—ï¼‰
    attn_output = torch.randn(all_q.shape[0], NUM_HEADS, KV_DIM, device=device)
    print(f"[RANK {rank}] attn_output (GEMM) -> {attn_output.shape}",flush=True)

    # 8) æ‹†åˆ†ç»“æœ
    local_cnt   = local_q.shape[0]
    local_res   = attn_output[:local_cnt]  # Localè¯·æ±‚ç»“æœï¼ˆæ— éœ€åˆå¹¶ï¼‰
    sp_res      = attn_output[local_cnt:]  # SPè¯·æ±‚ç»“æœï¼ˆéœ€All2Allååˆå¹¶ï¼‰
    print(f"[RANK {rank}] local_res={local_res.shape}, sp_res={sp_res.shape}",flush=True)

    # 9) All2All æ‹†åˆ† SP ç»“æœï¼ˆæ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ï¼‰
    final_sp_parts = []
    sp_ptr = 0
    for key, m in meta.items():
        comm        = sp_comm_groups[key]
        ranks       = m['group_ranks']  # SPç»„å†…æ‰€æœ‰Rank
        all_cnts    = m['all_batch_counts']  # ç»„å†…æ¯ä¸ªRankçš„åŸå§‹SPè¯·æ±‚æ•°
        my_sp_cnt   = m['local_batch_count']  # å½“å‰Rankçš„SPè¯·æ±‚æ•°ï¼ˆéœ€åˆå¹¶çš„è¯·æ±‚æ•°ï¼‰
        rank_idx    = ranks.index(rank)
        total_sp    = sum(all_cnts)  # ç»„å†…æ‰€æœ‰SPè¯·æ±‚æ€»æ•°

        # æˆªå–å½“å‰SPç»„å¯¹åº”çš„ç»“æœåˆ‡ç‰‡
        slice_sp = sp_res[sp_ptr:sp_ptr + total_sp]
        sp_ptr  += total_sp
        print(f"[RANK {rank}] slice_sp for {key} -> {slice_sp.shape}",flush=True)

        # æ„é€ å‘é€/æ¥æ”¶é•¿åº¦ï¼ˆsend_cntsï¼šå‘ç»™æ¯ä¸ªRankçš„æ•°é‡ï¼›recv_cntsï¼šä»æ¯ä¸ªRankæ¥æ”¶çš„æ•°é‡ï¼‰
        send_cnts = all_cnts  # å‘ç»™Rank içš„æ•°é‡ = Rank içš„åŸå§‹SPè¯·æ±‚æ•°
        recv_cnts = [all_cnts[rank_idx] for _ in ranks]  # ä»æ¯ä¸ªRankæ¥æ”¶çš„æ•°é‡ = å½“å‰Rankçš„SPè¯·æ±‚æ•°
        print(f"[RANK {rank}] send_cnts={send_cnts}, recv_cnts={recv_cnts}",flush=True)

        # æ„é€ å‘é€åˆ—è¡¨ï¼ˆæŒ‰Rankæ‹†åˆ†slice_spï¼‰
        send_list, pos = [], 0
        for c in send_cnts:
            end = pos + c
            if c > 0 and slice_sp.numel() > 0:
                send_list.append(slice_sp[pos:end].contiguous())
            else:
                send_list.append(torch.empty(0, NUM_HEADS, KV_DIM, device=device))
            pos = end

        # æ„é€ æ¥æ”¶åˆ—è¡¨ï¼ˆé¢„åˆ†é…å†…å­˜ï¼‰
        recv_list = []
        for c in recv_cnts:
            if c > 0:
                recv_tensor = torch.empty(c, NUM_HEADS, KV_DIM, dtype=torch.float32, device=device)
                recv_list.append(recv_tensor)
            else:
                recv_list.append(torch.empty(0, NUM_HEADS, KV_DIM, device=device))

        # æ‰§è¡ŒAll2Allé€šä¿¡ï¼ˆäº¤æ¢SPè¯·æ±‚ç»“æœï¼‰
        dist.all_to_all(recv_list, send_list, group=comm)

        # -------------------------- æ ¸å¿ƒä¿®æ”¹ï¼šSPè¯·æ±‚ç»“æœåˆå¹¶ --------------------------
        merged_sp_results = []
        if my_sp_cnt > 0:
            # 1. åˆå¹¶æ¥æ”¶çš„æ‰€æœ‰éç©ºå¼ é‡ï¼ˆæŒ‰Rankç»´åº¦ï¼‰
            sp_results_received = []
            for tensor in recv_list:
                if tensor.numel() > 0:
                    sp_results_received.append(tensor)
            sp_results_received = torch.cat(sp_results_received, dim=0) if sp_results_received else \
                                  torch.empty(0, NUM_HEADS, KV_DIM, device=device)
            print(f"[RANK {rank}] all2all received sp results -> {sp_results_received.shape}",flush=True)

            # 2. ä¸ºæ¯ä¸ªSPè¯·æ±‚åˆ†é…æ‰€æœ‰Rankçš„ç»“æœï¼ˆæŒ‰å‘é€æ–¹Rankæ‹†åˆ†ï¼‰
            sp_request_results = [[] for _ in range(my_sp_cnt)]  # æ¯ä¸ªè¯·æ±‚å¯¹åº”ä¸€ä¸ªç»“æœåˆ—è¡¨
            current_pos = 0
            for i in range(len(ranks)):
                recv_c = recv_cnts[i]  # ä»Rank iæ¥æ”¶çš„æ•°é‡
                if recv_c > 0:
                    end_pos = current_pos + recv_c
                    # å°†Rank içš„ç»“æœåˆ†é…åˆ°å¯¹åº”è¯·æ±‚çš„åˆ—è¡¨ä¸­
                    for req_idx in range(recv_c):
                        sp_request_results[req_idx].append(sp_results_received[current_pos + req_idx])
                    current_pos = end_pos

            # 3. åˆå¹¶æ¯ä¸ªSPè¯·æ±‚çš„æ‰€æœ‰Rankç»“æœï¼ˆä½¿ç”¨dummy_mergeï¼‰
            for req_idx in range(my_sp_cnt):
                merged = dummy_merge(sp_request_results[req_idx])
                merged_sp_results.append(merged)

            # 4. è½¬ä¸ºå¼ é‡ï¼ˆmy_sp_cnt, NUM_HEADS, KV_DIMï¼‰
            merged_sp_tensor = torch.stack(merged_sp_results, dim=0)
            print(f"[RANK {rank}] merged sp results -> {merged_sp_tensor.shape}",flush=True)
            final_sp_parts.append(merged_sp_tensor)
        else:
            # æ— SPè¯·æ±‚æ—¶æ·»åŠ ç©ºå¼ é‡
            final_sp_parts.append(torch.empty(0, NUM_HEADS, KV_DIM, device=device))

    # 10) æœ€ç»ˆè¾“å‡º
    final_sp = torch.cat(final_sp_parts) if final_sp_parts else \
               torch.empty(0, NUM_HEADS, KV_DIM, device=device)
    final    = torch.cat([local_res, final_sp], dim=0)  # Localç»“æœ + åˆå¹¶åçš„SPç»“æœ
    print(f"[RANK {rank}] final_sp={final_sp.shape}, final={final.shape}",flush=True)
    print(f"========== [RANK {rank}] DONE ==========\n",flush=True)
    return final, meta

# -------------------------- åˆ†å¸ƒå¼æµ‹è¯• --------------------------
def init_distributed(rank: int, world_size: int):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12362'
    dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def verify_results(rank: int, final_output: torch.Tensor, sp_groups_info: Dict) -> bool:
    # è®¡ç®—é¢„æœŸç»“æœæ•°é‡ï¼šlocalè¯·æ±‚æ•° + å½“å‰Rankçš„SPè¯·æ±‚æ•°
    expected_cnt = 0
    for b, info in sp_groups_info.items():
        if not info['enabled']:
            expected_cnt += 1  # Localè¯·æ±‚
        elif rank in info['group']:
            expected_cnt += 1  # å½“å‰Rankçš„SPè¯·æ±‚
    expected_shape = (expected_cnt, NUM_HEADS, KV_DIM)
    
    # æ ¡éªŒå½¢çŠ¶å’Œéé›¶ï¼ˆæ’é™¤å…¨é›¶æ— æ•ˆç»“æœï¼‰
    if final_output.shape != expected_shape:
        print(f"âŒ Rank {rank} æœŸæœ› {expected_shape}, å®é™… {final_output.shape}",flush=True)
        return False
    if final_output.numel() > 0 and torch.all(final_output == 0):
        print(f"âŒ Rank {rank} è¾“å‡ºå…¨é›¶",flush=True)
        return False
    print(f"âœ… Rank {rank} æ ¡éªŒé€šè¿‡",flush=True)
    return True

def test_forward(rank: int, world_size: int):
    init_distributed(rank, world_size)
    device = torch.device(f'cuda:{rank}')
    hidden_states = torch.randn(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, device=device)

    # æ¯ä¸ªRankçš„SPç»„é…ç½®ï¼šBatch 0/1ä¸ºSPè¯·æ±‚ï¼ˆç»„[0,1,2,3]ï¼‰ï¼ŒBatch 2/3ä¸ºLocalè¯·æ±‚
    sp_groups_info_list = [
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 0},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 0},
         2: {'enabled': False}, 3: {'enabled': False}},
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 1},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 1},
         2: {'enabled': False}, 3: {'enabled': False}},
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 2},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 2},
         2: {'enabled': False}, 3: {'enabled': False}},
        {0: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 3},
         1: {'enabled': True, 'group': [0,1,2,3], 'master_rank': 3},
         2: {'enabled': False}, 3: {'enabled': False}},
    ]

    # é¢„åˆ›å»ºSPé€šä¿¡ç»„ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
    sp_comm_groups = {}
    if rank in [0,1,2,3]:
        sp_comm_groups[tuple(sorted([0,1,2,3]))] = dist.new_group(ranks=[0,1,2,3])

    # æ‰§è¡Œforward
    final_output, _ = forward(
        hidden_states=hidden_states,
        sp_groups_info=sp_groups_info_list[rank],
        sp_comm_groups=sp_comm_groups
    )

    # æ ¡éªŒç»“æœ
    ok = verify_results(rank, final_output, sp_groups_info_list[rank])
    dist.barrier()

    # æ±‡æ€»æ‰€æœ‰Rankçš„æ ¡éªŒç»“æœ
    if rank == 0:
        all_ok = torch.tensor(1, device='cuda:0')
        for r in range(1, world_size):
            buf = torch.tensor(0, device='cuda:0')
            dist.recv(buf, src=r)
            all_ok &= buf
        print("\nğŸ‰ æ‰€æœ‰Rankæ ¡éªŒé€šè¿‡ï¼" if all_ok else "\nâŒ éƒ¨åˆ†Rankæ ¡éªŒå¤±è´¥ï¼",flush=True)
    else:
        dist.send(torch.tensor(1 if ok else 0, device='cuda'), dst=0)
    
    dist.barrier()
    dist.destroy_process_group()

def main():
    WORLD_SIZE = 4  # 4å¡åˆ†å¸ƒå¼æµ‹è¯•
    spawn(fn=test_forward, args=(WORLD_SIZE,), nprocs=WORLD_SIZE, join=True)

if __name__ == '__main__':
    main()